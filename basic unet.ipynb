{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "step3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdHVQrqyDB50"
      },
      "source": [
        "# Memory Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgVfBlvZDEja"
      },
      "source": [
        "# GPU Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyDODoaWC6KI"
      },
      "source": [
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsPnAFcmkLA5"
      },
      "source": [
        "import kaggle\n",
        "import os\n",
        "\n",
        "\n",
        "dataset_dir = '/content/home/'\n",
        "def download_drive(dataset_dir):\n",
        "    \"\"\"\n",
        "    Downloads dataset from Kaggle and loads it in dataset_dir.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        kaggle.api.authenticate()\n",
        "        kaggle.api.dataset_download_files(dataset=\"mbonyani/segmentdatactc\", path=dataset_dir, unzip=True)\n",
        "        print('Download completed.')\n",
        "    else:\n",
        "        print('dataset already exists.')\n",
        "    \n",
        "    return True\n",
        "download_drive(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk4-JI8PACJe"
      },
      "source": [
        "!pip install tensorflow_addons==0.11.2\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib\r\n",
        "import pandas as pnd\r\n",
        "from tqdm import tqdm\r\n",
        "import sys\r\n",
        "import math\r\n",
        "import os\r\n",
        "import zipfile\r\n",
        "import six\r\n",
        "import warnings\r\n",
        "import random\r\n",
        "import gc\r\n",
        "import six\r\n",
        "from math import ceil\r\n",
        "from sklearn.model_selection import KFold, StratifiedKFold,train_test_split\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "from keras.metrics import top_k_categorical_accuracy\r\n",
        "\r\n",
        "\r\n",
        "import cv2\r\n",
        "import imgaug as ia\r\n",
        "from imgaug import augmenters as iaa\r\n",
        "from moviepy.editor import VideoFileClip\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "\r\n",
        "from scipy.io import loadmat\r\n",
        "import keras.backend as K\r\n",
        "from keras.engine.topology import get_source_inputs\r\n",
        "from keras.layers import Activation\r\n",
        "from keras.layers import AveragePooling3D\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from keras.layers import Conv3D\r\n",
        "from keras.layers import Conv3DTranspose\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout,Flatten\r\n",
        "from keras.layers import GlobalAveragePooling3D\r\n",
        "from keras.layers import GlobalMaxPooling3D\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import MaxPooling3D\r\n",
        "from keras.layers import Reshape\r\n",
        "from keras.layers import UpSampling3D\r\n",
        "from keras.layers import Concatenate\r\n",
        "from tensorflow.keras.models import Model,load_model\r\n",
        "from keras.regularizers import l2\r\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\r\n",
        "from keras_contrib.layers import SubPixelUpscaling\r\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\r\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n",
        "from tensorflow.keras.optimizers import SGD,Adam\r\n",
        "\r\n",
        "import tensorflow\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from tensorflow.keras import backend as keras\r\n",
        "\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from tensorflow.keras.optimizers import *\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.losses import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_KGTU-VACHD"
      },
      "source": [
        "import gc \r\n",
        "\r\n",
        "X_train = np.load('/content/home/train_img.npy').reshape((19637, 128, 128,1))\r\n",
        "y_train = np.load('/content/home/train_seg.npy').reshape((19637, 128, 128,1))\r\n",
        "\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "X_val = np.load('/content/home/val_img.npy').reshape((7521, 128, 128,1))\r\n",
        "y_val = np.load('/content/home/val_seg.npy').reshape((7521, 128, 128,1))\r\n",
        "\r\n",
        "# X_test = np.load('/content/home/data_road_asl.npy')\r\n",
        "# y_test = np.load('/content/home/data_face_asl.npy')\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(y_train.shape)\r\n",
        "\r\n",
        "print(X_val.shape)\r\n",
        "print(y_val.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HQi4xlUACEW"
      },
      "source": [
        "def closs(y_true, y_pred):\r\n",
        "    def dice_loss(y_true, y_pred):\r\n",
        "        numerator = 2 * tensorflow.reduce_sum(y_true * y_pred, axis=(1,2,3))\r\n",
        "        denominator = tensorflow.reduce_sum(y_true + y_pred, axis=(1,2,3))\r\n",
        "\r\n",
        "        return K.reshape(1 - numerator / denominator, (-1, 1, 1))\r\n",
        "\r\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\r\n",
        "  \r\n",
        "def iou_core(y_true, y_pred, smooth=1):\r\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\r\n",
        "    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\r\n",
        "    iou = (intersection + smooth) / ( union + smooth)\r\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iwr8iNGACB3"
      },
      "source": [
        "import numpy as np \r\n",
        "import os\r\n",
        "import skimage.io as io\r\n",
        "import skimage.transform as trans\r\n",
        "import numpy as np\r\n",
        "from keras.models import *\r\n",
        "from keras.layers import *\r\n",
        "from keras.optimizers import *\r\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\n",
        "from keras import backend as keras\r\n",
        "\r\n",
        "\r\n",
        "def unet(pretrained_weights = None,input_size = (128,128,1)):\r\n",
        "    inputs = Input(input_size)\r\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\r\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\r\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\r\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\r\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\r\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\r\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\r\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\r\n",
        "    drop4 = Dropout(0.5)(conv4)\r\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\r\n",
        "\r\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\r\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\r\n",
        "    drop5 = Dropout(0.5)(conv5)\r\n",
        "\r\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\r\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\r\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\r\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\r\n",
        "\r\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\r\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\r\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\r\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\r\n",
        "\r\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\r\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\r\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\r\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\r\n",
        "\r\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\r\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\r\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\r\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\r\n",
        "\r\n",
        "    model = Model(inputs = inputs, outputs = conv10)\r\n",
        "\r\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n",
        "    \r\n",
        "    #model.summary()\r\n",
        "\r\n",
        "    if(pretrained_weights):\r\n",
        "    \tmodel.load_weights(pretrained_weights)\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVfYqzdQAB_M"
      },
      "source": [
        "model = unet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m23NFvAUAB8a"
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\r\n",
        "model.fit(X_train,y_train, epochs=30,batch_size=16, validation_data=(X_val, y_val),callbacks=[model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}